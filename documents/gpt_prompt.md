
## Conversation 1
本对话中，你是一个经验丰富的的量化交易员、同时是一个理论基础扎实的深度学习研究员，面对的任务环境是美股标的股票与期权交易，任务目标是设计一个确信度优秀的模型、配合交易策略与交易指引（都需要自行构建）实现盈利。模型可以是 N-BEATS、N-BEATSx、N-HiTS、FEDformer、DeepTiMe、Transformer Based 等DeepLearning Based 模型，也可以是 XGBoost 等经典的时序模型，完全根据任务需求而决定。

以SPY为主要标的，请你真正编程实现一套量化交易系统，主要用于在周内跨日（主要是标的股票）和日内短线（主要是期权）交易中，给出明确的交易信号。

系统应包含一下几点
1. 数据准备
2. 模型构建
3. 模型训练与评估
4. 交易策略设计
5. 实时监控、实时推理与可视化、交易信号生成
6. 模型的增量学习、迭代与优化

对于第一点的数据准备而言，数据来源为 Interactive Brokers TWS API，我已经订阅了 SPY: Arca 的 L1 基础数据和 L2 盘口深度（交易量）数据，你被要求完成两个程序：获取历史数据和获取实时数据。对于历史数据：
1. 检索 data/daily_data 路径，若该路径下没有有效文件，则获取从今天开始过去120个交易日的分钟级L1数据
2. 若该路径下有数据，则获取从现有的最后一天的文件直到今天的分钟级数据
3. 获取到的原始数据保存到 data/original_data 中，然后进行特征工程、构造高级技术指标
4. 高级技术指标现在包含[]，你被要求根据你的理解和知识，自行删减增加；需要注意的一点是，高级指标一定要有效，尽可能规避出现相似度较高的重复项
5. 将数据的第一列（索引列）命名为 Datetime ，并保持其作为索引列
6. 特征工程结束后的数据按天切分、每天的数据以csv文件独立保存到 daily_data 路径下

对于实时数据：
0. 使用 C++ 实现。
1. 实现完整的头文件、源文件、Log日志系统、链接断开重试机制、其他错误应对机制，保证程序的健壮性
2. 程序组织结构要合理美观，切忌将所有的功能实现在同一个源文件里
3. 给出除了源文件外的 CMakeLists.txt 和 Make 编译步骤
4. 与 IB TWS API 的交互使用交易所所在的美东（纽约）时间
5. 程序在 While True 里面不间断的运行，当SPY 未开盘则睡眠，开盘后，每秒（应当是一个严格的要求，时间间隔的浮动是可以接受的，但应保证不要过于剧烈）或者更短时间请求一次L1(Open/CLose/High/Low/Volume) 和 L2(盘口交易量、在什么价位发生了多少笔买入或卖出)实时数据
6. 获取到的原始数据持续不断地写入 data/realtime_original_data/ 中的 csv 文件中
7. 对数据进行可以实时计算的的特征工程（这意味着不可以进行计算量大到影响是实行的指标构建），且尽可能和历史数据的特征构造相同
8. 将L1数据、L2数据、和特征工程后的数据持续不断地写入 data/daily_realtime_data/ 中的当天的 csv 文件中
9. 预留实时数据流的 Socket 接口，以供后续 Python 程序进行实时监控、实时推理、实时可视化、实时信号生成、模型增量训练使用
10. 预留实时数据流的 Shared Memory 接口，以供后续 Python 程序进行实时监控、实时推理、实时可视化、实时信号生成、模型增量训练使用
11. 实现程序常驻后台，以便持续不断地获取数据

对于第二、第三部分的模型构建与训练，你不被允许进行示例性的实现，而要充分考虑下列要求：
1. 认真考虑和充分利用数据特征
2. 认真且充分考虑金融市场数据的特点
3. 认真考虑和充分专注于日内短线的数据表现
4. 认真且充分考虑损失构建的全面性和模型衡量指标的合理性
5. 充分保证代码的完成度，实现足够健壮的程序代码。
6. 从而作出高完成度的、健壮的、可准确捕捉市场变动、走势与趋势的代码实现。

对第五点中的交易信号生成，我们的要求是“简洁清晰”。具体而言：
1. 给出确信度足够高的买入或者卖出（包括标的股票的买卖、期权的Long Call/Long Put）信号指引，而尽可能少的展示其他无关信号
2. 如果进行标的股票跨日交易，应当在一个时间范围内给出买入与卖出指引
3. 如果进行期权日内短线交易，则应当提前数分钟到数十分钟给出如下信息：在标的股票到达什么价位时进行行权价为多少的买入开仓（Call 或 Put），又应该在什么价位卖出平仓，止盈与止损怎么设置等

对第五点中的实时监控、实时推理与可视化和第六点中的增量学习、迭代与优化，我们指的是：
1. 每分钟或者更短时间级别获取一次实时数据，根据历史一段时间（input_window）的数据（L1/L2/技术指标等）对未来一段时间（output_window）的标的走势进行预测，并进行预测结果的可视化；可视化需要实时更新，每进行一次推理就进行一次更新
2. 每分钟或者更短时间级别获取一次实时数据，根据最新的数据对模型进行微调，使模型学习拟合最新的数据、学习最新的数据走势趋势
3. 每分钟或者更短时间级别获取一次实时数据，进行和第一部分相同的特征工程，并以相同的命名格式持续写入到 data/daily_data/ 中今天的数据 csv 文件中


需要特别指出的是，我们的指引构建所使用的全部数据都严格禁止存在非实际的假设或者示例数据，如果这个指标很重要，那么数据务必根据标的各种指标进行算法分析得到；否则应该摒弃。指引一定要是实盘可用的，而不能只是简单的示例。

最后，向你强调任务目标：给出高确信度的交易信号（包括标的股票和期权），其中期权交易需要更精准的指引：提前数分钟到数十分钟给出在标的股票到达什么价位时进行行权价为多少的买入开仓（Call 或 Put），又应该在什么价位卖出平仓。

在你理解要求和目标之后，不必有太多其他拘束：依照第一性原理去思考，只要能达到预设的要求与目标，其他的一切你可以自由发挥。

当然，这个项目实在太过庞大，不可能使用短短几百到几千行代码实现，所以我们一步一步地进行，共同完成。

现在，我希望你整体理解这个工程，所以请你按照你的理解，复述我们的任务目标和要求和实现思路。


## Conversation 2
【文件】get_data.py
【文件】交易日数据

非常好。接下来让我们一起努力，一步一步地去完成这个任务。

首先是数据准备。我将发送给你一份现有的基于其他 API 的数据构建的代码文件 get_data.py，要求你完整而仔细的阅读、并深入理解；我将发送你一个现有的切分后的某交易日的csv 数据文件，作为当下数据构成的参考。

当阅读和理解后，作为经验丰富的的量化交易员、同时是一个理论基础扎实的深度学习研究员，请你以这个代码文件（尤其是对特征的构造和数据的组织保存等部分）为基础，更换为基于 IB TWS API 的数据获取，面向我们的任务要求与目标，根据你的知识和理解，进行增加、删除、修改等操作，自行完善数据处理、指标构建、特征工程等工作。

在数据准备着一部分，数据来源为 Interactive Brokers TWS API，我已经订阅了 SPY: Arca 的 L1 基础数据和 L2 盘口深度（交易量）数据，你被要求完成两个程序：获取历史数据和获取实时数据。现在，我们先来实现第一个历史数据获取、特征工程、和切分保存的程序。我们强调：
0. 使用 python 实现
1. 检索 data/daily_data 路径，若该路径下没有有效文件，则获取从今天开始过去120个交易日的分钟级L1数据
2. 若该路径下有数据，则获取从现有的最后一天的文件直到今天的分钟级数据
3. 获取到的原始数据保存到 data/original_data 中，然后进行特征工程、构造高级技术指标
4. 你被要求根据你的理解和知识，自行删减增加；需要注意的一点是，高级指标一定要有效，尽可能规避出现相似度较高的重复项
5. 将数据的第一列（索引列）命名为 Datetime ，并保持其作为索引列
6. 特征工程结束后的数据按天切分、每天的数据以csv文件独立保存到 daily_data 路径下

请回顾任务环境与目标，仔细理解需求，然后给出完整的程序实现。

## Conversation 3
恭喜你，代码是可以运行的，也是符合预期的。那么接下来，让我们一起实现获取实时数据的程序。回顾要求如下：
0. 使用 C++ 实现。
1. 实现完整的头文件、源文件、Log日志系统、链接断开重试机制、其他错误应对机制，保证程序的健壮性
2. 程序组织结构要合理美观，切忌将所有的功能实现在同一个源文件里
3. 给出除了源文件外的 CMakeLists.txt 和 Make 编译步骤
4. 与 IB TWS API 的交互使用交易所所在的美东（纽约）时间
5. 程序在 While True 里面不间断的运行，当SPY 未开盘则睡眠，开盘后，每秒（应当是一个严格的要求，时间间隔的浮动是可以接受的，但应保证不要过于剧烈）或者更短时间请求一次L1(Open/CLose/High/Low/Volume) 和 L2(盘口交易量、在什么价位发生了多少笔买入或卖出)实时数据
6. 获取到的原始数据持续不断地写入 data/realtime_original_data/ 中的 csv 文件中
7. 对数据进行可以实时计算的的特征工程（这意味着不可以进行计算量大到影响是实行的指标构建），且尽可能和历史数据的特征构造相同
8. 将L1数据、L2数据、和特征工程后的数据持续不断地写入 data/daily_realtime_data/ 中的当天的 csv 文件中
9. 预留实时数据流的 Shared Memory 接口，以供后续 Python 程序进行实时监控、实时推理、实时可视化、实时信号生成、模型增量训练使用
10. 实现程序常驻后台，以便持续不断地获取数据

请回顾任务环境与目标，仔细理解需求，然后给出完整的程序实现。